---
title: "Model Optimization"
date: 2024-01-12T14:34:17+05:45
draft: false

cover:
    image: 'img/optimization.png'
    alt: 'Model Optimization'
    caption: 'Read More....'
    relative: false
    hidden: false
tags: ["ai", "ml"]
categories: ["tech", "optimization"]
---


# Model Optimization

Most deep learning models are made up of millions/billions of parameters. So, when we inference a model, we need to load all its parameter in the memory, this means big models cannot be loaded easily on a edge/embedded devices. So, this blog is focused on optimizing a model through different techniques and analyze its effect on inference time, model size, accuracy.

Model optimization is transforming machine/deep learning model such that it has:

1. Faster Inference Time

2. Reduction in network size (memory efficient)

We're going to cover different topics, so buckle up

1. Quantization
Reducing number of bits used for model parameters

2. Network Pruning
Reducing the number of distinct weight values

3. Low-Rank Matrix Factorization
Factorizing model weights such that it reduces overall model parameters

4. Knoweldge Distillation 
Obtatining a smaller netwokr by mimicking the prediction


![Optimization](https://raw.githubusercontent.com/shulavkarki/shulavkarki.github.io/master/static/img/model_optimization/types_diagram.png)